{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Topic Modeling: An Application'\nauthor: Salfo Bikienga\ndate: '2017-11-11'\ncategories:\n  - R\n  - Text Analytics\n  - Topic Modeling\n  - LDA\ntags:\n  - R\n  - LDA\n  - Text Analytics\n  - Topic Modeling\nslug: topic-modeling-an-application\n---\n\n# Introduction\n\nMy work involves the use and the development of topic modeling algorithms. A surprising challenge I have had is communicating the output of topic modeling algorithms to people not familiar with text analytics. Here is my 10 cents explanation of the LDA output to my econ friends.\n\nThe use of text data for <a href=\"http://review.chicagobooth.edu/magazine/spring-2015/why-words-are-the-new-numbers\" target=\"_blank\">economic analysis</a> is gaining attractions. One popular analytical tool is Latent Dirichlet Allocation (LDA), also called topic modeling [@Blei2003]. Succinctly put, topic modeling consists of collapsing a matrix (i.e a spreadsheet) of words counts into a reduced matrix of topics' proportions within documents. For instance, assume we have a collection of 500 documents, each containing 2000 unique words; this collection of documents (called corpus) can be represented as a dataset of 500 observations and 2000 variables (each word being a variable). Each cell in the matrix represents the count of a word in a document. The matrix is just a regular spreadsheet of data. Clearly, it is almost impossible to draw any insight from that many variables. LDA allows us to collapse the high dimensional dataset into a lower dimension, say a dimension of 10. With 10 variables, there is a hope that some insight can be drawn from the data. Following is a demonstration of LDA.\n\n# Example Data\n\nLet's consider a dataset of U.S. governors' State of the State Addresses (SoSA). In most states, the governor gives a speech, generally in January, in which he/she lays out his/her priorities for the next fiscal year. Part of the goal of the speech is to explain (or justify) the proposed budget, and hopefully convince the state stakeholders to support the proposed budget. A budget proposal usually involves a reallocation of the state resources, which implies cuts and increases in different lines of the state budget.\nI collected 596 speeches from governors of the 50 states, spanning from 2001 to 2013.\n\nIt is customary in text analytics to delete words that we believe are not \"discriminative\". For instance link words such as \"the\", \"and\", \"she\", etc. will not distinguish a Democrat from a Republican. We call this process, pre-processing the data, that is, cleaning the data by removing elements in the texts that we believe are not useful for our analysis.\n\nAfter pre-processing the data, I am left with a dataset of 596 observations and 1034 words (or variables). You can take a look at the pre-processed data <a href=\"http://rpubs.com/sbikienga/334137\" target=\"_blank\">here</a>, or you can download it <a href=\"https://github.com/Salfo/States-Addresses/raw/master/data/SoSA_data_df.RData\" target=\"_blank\">here</a>. Stemming, that is stripping the words to their roots, is often done to avoid counting related words separately. For example, education, educational, educate are stemmed and become educ.\n\n# Example application of LDA\n\nThe goal when using LDA is primarily to reduce the dimension of a counts dataset. The hope is that the reduced dimension preserves the essential information contained in the original dataset. Interestingly, the reduced dimension is often more appropriate for statistical analysis, as it \"solves\" the overfitting problem associated with high dimensional data. Generally, the overfitting problem arises in situations where $n$, the number of observations, is not big enough to provide reliable estimates of the $p$ variables' parameters.\n\nThere are several packages in R to implement the LDA model (`lda`, `mallet`, and `topicmodels`). Here I will use the `topicmodels` package as an example.\n\n#Conclusion\n\nIn sum, topic modeling in general and LDA in particular is a dimension reduction method. It consists of collapsing a matrix of words counts into a reduced matrix of topics distributions. This illustration provides a sense of its usefulness for statistical analysis. \n\n\n# References\n\n",
    "created" : 1526653915237.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1914749134",
    "id" : "D583D2DB",
    "lastKnownWriteTime" : 1526653987,
    "last_content_update" : 1526653987860,
    "path" : "D:/AymanAminWebsite/aymanamin/content/post/2017-11-11-topic-modeling-an-application.Rmd",
    "project_path" : "content/post/2017-11-11-topic-modeling-an-application.Rmd",
    "properties" : {
    },
    "relative_order" : 14,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}